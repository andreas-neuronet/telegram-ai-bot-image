from gradio_client import Client
import os
from dotenv import load_dotenv
from PIL import Image

# === –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–æ–∫–µ–Ω–∞ –∏–∑ .env ===
load_dotenv()
HF_TOKEN = os.getenv("HF_TOKEN")

if not HF_TOKEN:
    raise ValueError("‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω HF_TOKEN –≤ .env —Ñ–∞–π–ª–µ.")

# === –ù–∞—Å—Ç—Ä–æ–π–∫–∏ ===
MODEL_FILE = "MODEL.txt"
INPUT_FILENAME = "input.txt"
OUTPUT_DIR = "image"

# === –†–µ–∑–µ—Ä–≤–Ω—ã–µ –º–æ–¥–µ–ª–∏ ===
BACKUP_MODELS = [
    "black-forest-labs/FLUX.1-schnell",
    "stabilityai/stable-diffusion-xl-base-1.0",
    "runwayml/stable-diffusion-v1-5"
]

def get_working_model():
    if os.path.exists(MODEL_FILE):
        try:
            with open(MODEL_FILE, "r", encoding="utf-8") as f:
                models = [line.strip() for line in f if line.strip()]
                for model in models:
                    try:
                        print(f"üîÑ –ü—Ä–æ–≤–µ—Ä–∫–∞ –º–æ–¥–µ–ª–∏: {model}")
                        Client(model, hf_token=HF_TOKEN)
                        return model
                    except Exception as e:
                        print(f"‚ùå –ú–æ–¥–µ–ª—å {model} –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞: {e}")
                        continue
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è —Ñ–∞–π–ª–∞ –º–æ–¥–µ–ª–µ–π: {e}")

    print("‚ö†Ô∏è –ò—Å–ø–æ–ª—å–∑—É—é —Ä–µ–∑–µ—Ä–≤–Ω—ã–µ –º–æ–¥–µ–ª–∏...")
    for model in BACKUP_MODELS:
        try:
            Client(model, hf_token=HF_TOKEN)
            return model
        except Exception as e:
            print(f"‚ùå –†–µ–∑–µ—Ä–≤–Ω–∞—è –º–æ–¥–µ–ª—å {model} –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞: {e}")
            continue

    raise ValueError("‚ùå –ù–∏ –æ–¥–Ω–∞ –∏–∑ –º–æ–¥–µ–ª–µ–π –Ω–µ –¥–æ—Å—Ç—É–ø–Ω–∞!")

# === –û—Å–Ω–æ–≤–Ω–æ–π –ø—Ä–æ—Ü–µ—Å—Å ===
MODEL_NAME = get_working_model()
print(f"‚úÖ –ò—Å–ø–æ–ª—å–∑—É—é –º–æ–¥–µ–ª—å: {MODEL_NAME}")
client = Client(MODEL_NAME, hf_token=HF_TOKEN)
os.makedirs(OUTPUT_DIR, exist_ok=True)

try:
    with open(INPUT_FILENAME, "r", encoding="utf-8") as f:
        prompts = [line.strip() for line in f if line.strip()]
except FileNotFoundError:
    print(f"‚ùå –§–∞–π–ª {INPUT_FILENAME} –Ω–µ –Ω–∞–π–¥–µ–Ω.")
    exit()

for idx, prompt in enumerate(prompts):
    print(f"\nüîÑ [{idx + 1}/{len(prompts)}] –ì–µ–Ω–µ—Ä–∞—Ü–∏—è: ¬´{prompt}¬ª")
    try:
        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è FLUX.1-schnell
        if "FLUX.1-schnell" in MODEL_NAME:
            result = client.predict(
                prompt=prompt,
                api_name="/infer"
            )
        else:  # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –¥—Ä—É–≥–∏—Ö –º–æ–¥–µ–ª–µ–π
            result = client.predict(
                prompt=prompt,
                seed=0,
                width=1024,
                height=1024,
                guidance_scale=3.5,
                num_inference_steps=28,
                api_name="/infer"
            )

        temp_image_path = result[0]
        safe_prompt = "".join(c for c in prompt[:30] if c.isalnum() or c in " _-")
        output_path = os.path.join(OUTPUT_DIR, f"{idx + 1}_{safe_prompt}.png")

        # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è
        with Image.open(temp_image_path) as img:
            img.save(output_path, "PNG", quality=100)
        print(f"üñºÔ∏è –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ –∫–∞–∫ PNG: {output_path}")

    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞: {str(e)}")

print("\nüéâ –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")

import requests

TELEGRAM_BOT_TOKEN = os.getenv("TELEGRAM_BOT_TOKEN")
TELEGRAM_CHANNEL_ID = os.getenv("TELEGRAM_CHANNEL_ID")

def send_to_telegram(image_path, caption=""):
    url = f"https://api.telegram.org/bot {TELEGRAM_BOT_TOKEN}/sendPhoto"
    with open(image_path, "rb") as photo:
        files = {"photo": photo}
        data = {"chat_id": TELEGRAM_CHANNEL_ID, "caption": caption}
        response = requests.post(url, data=data, files=files)
    return response.json()

# –ü–æ—Å–ª–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
send_to_telegram(output_path, caption=f"üñºÔ∏è {prompt}")